{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxyxb\\AppData\\Local\\Continuum\\anaconda3\\envs\\consensus_BO_39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "tensor(0.9775367996)\n",
      "tensor(0.9629900014)\n",
      "tensor(0.9451589468)\n",
      "tensor(0.9815509197)\n",
      "tensor(0.9211800458)\n"
     ]
    }
   ],
   "source": [
    "## Levy\n",
    "\n",
    "######### install botorch library ##########\n",
    "\n",
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.utils import standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition import qMaxValueEntropy\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.test_functions import Levy\n",
    "from botorch.optim.initializers import initialize_q_batch_nonneg\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import random\n",
    "from random import choices\n",
    "import matplotlib.pyplot as plt\n",
    "from botorch.utils.transforms import standardize, normalize, unnormalize\n",
    "\n",
    "torch.set_default_dtype(torch.float64) # change data type to float64\n",
    "torch.set_printoptions(precision=10) # change print precision\n",
    "\n",
    "result = []\n",
    "\n",
    "for ii in range(0,1):\n",
    "    # ==================================\n",
    "    # reward weight --- Ackley\n",
    "    # ==================================\n",
    "\n",
    "    num_dev = 5 # number of device\n",
    "    input_dim = 8 # dimension of X\n",
    "    num_sample = choices(range(input_dim*5,input_dim*5+1), k=num_dev) # random.sample(range(10, 25), num_dev)# number of initial sample\n",
    "    # lr = 0.01 # learning rate\n",
    "\n",
    "    I_matrix = np.eye(input_dim) # I matrix\n",
    "    W_matrix_1 = ( np.ones((num_dev,num_dev)) ) / num_dev # np.eye(num_dev) # ( np.ones((num_dev,num_dev)) ) / num_dev # consensus matrix\n",
    "    W_matrix_2 = W_matrix_1 #( np.ones((num_dev,num_dev)) ) / num_dev\n",
    "\n",
    "    n_comm = n_comm_total = 20*input_dim\n",
    "    # n_comm = 15 # np.round(n_comm_total*0.7) #20 # number of communication round\n",
    "    # n_comm.astype(np.int64)\n",
    "    # n_ind_comm = n_comm_total - n_comm\n",
    "\n",
    "    bounds = torch.tensor([[-10., 10.], [-10., 10.], [-10., 10.] , [-10., 10.], [-10., 10.], [-10., 10.], [-10., 10.], [-10., 10.]]).T\n",
    "    # bounds = torch.tensor([[-33., 33.], [-33., 33.], [-33., 33.], [-33., 33.], [-33., 33.] ]).T\n",
    "    # bounds = torch.tensor([[-1., 1.], [-1., 1.], [-1., 1.], [-1., 1.], [-1., 1.] ]).T\n",
    "\n",
    "    # -------------------\n",
    "    # creating data\n",
    "    # -------------------\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    from operator import itemgetter # argmax\n",
    "\n",
    "    for i in range(0, num_dev):\n",
    "        globals()['a_%s' % i]  = torch.FloatTensor(1, 1).uniform_(0.5, 1) #np.random.uniform(0.5, 1, 1)\n",
    "        globals()['b_%s' % i]  = torch.empty(1).normal_(mean=0,std=1) #np.random.normal(0, 1, 1)\n",
    "        globals()['c_%s' % i]  = torch.empty(1).normal_(mean=0,std=1) #np.random.normal(0, 1, 1)\n",
    "        globals()['original_train_X_%s' % i] = globals()['temp_train_X_%s' % i] = globals()['train_X_%s' % i] = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(num_sample[i], input_dim) # globals() can create variable on-the-fly\n",
    "        globals()['train_X_%s' % i] = eval('train_X_%s' % i) + eval('c_%s' % i)\n",
    "        globals()['original_train_Y_%s' % i] = globals()['temp_train_Y_%s' % i] = globals()['train_Y_%s' % i] = Levy(dim=input_dim, negate=True)(eval('train_X_%d' % (i))).unsqueeze(-1) # eval() can directly call the variable on-the-fly\n",
    "        globals()['train_Y_%s' % i] =  eval('a_%d' % (i))*eval('train_Y_%d' % (i)) +  eval('b_%d' % (i))\n",
    "        globals()['current_opt_%s' % i] = eval('train_Y_%d' % (i)).max()\n",
    "        globals()['optimal_Y_%s' % i] = (torch.tensor(0) *  eval('a_%d' % (i)) +  eval('b_%d' % (i)))[0][0] \n",
    "\n",
    "        # globals()['train_X_%s' % i] = normalize(eval('original_train_X_%d' % (i)), bounds=bounds)\n",
    "\n",
    "\n",
    "\n",
    "    ### collaboration\n",
    "\n",
    "    for iter in range(0, n_comm_total):\n",
    "\n",
    "        print(iter)\n",
    "        if iter == 0: loss_vector = [] # store losses (i.e., negative AF) from all devices\n",
    "        # -------------------\n",
    "        # initialize model\n",
    "        # -------------------\n",
    "\n",
    "        for i in range(0, num_dev):\n",
    "            globals()['model_%s' % i] = SingleTaskGP(eval('train_X_%d' % (i)), eval('train_Y_%d' % (i)))\n",
    "            globals()['mll_%s' % i] = ExactMarginalLogLikelihood(eval('model_%d' % (i)).likelihood, eval('model_%d' % (i)))\n",
    "            fit_gpytorch_mll(eval('mll_%d' % (i)))\n",
    "            globals()['candidate_set_%s' % i] = bounds[0] + (bounds[1] - bounds[0]) * (torch.rand(1000, bounds.size(1), device=bounds.device, dtype=bounds.dtype))\n",
    "            globals()['qMES_%s' % i] = ExpectedImprovement(eval('model_%d' % (i)), best_f= eval('train_Y_%d' % (i)).max()) # qMaxValueEntropy(eval('model_%d' % (i)), eval('candidate_set_%d' % (i)))\n",
    "\n",
    "            N = 1\n",
    "            q = 1\n",
    "            d = input_dim\n",
    "            globals()['Xraw_%s' % i] = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(100 * N, q, d)\n",
    "            globals()['Yraw_%s' % i] = eval('qMES_%d' % (i))(eval('Xraw_%d' % (i)))  # acquisition function\n",
    "\n",
    "            if iter == 0:\n",
    "                globals()['X_%s' % i], globals()['loss_%s' % i] = optimize_acqf(\n",
    "                    eval('qMES_%d' % (i)), bounds=bounds, q=1, num_restarts=10, raw_samples=512,\n",
    "                )\n",
    "                loss_vector.append(eval('loss_%d' % (i)))\n",
    "            # if iter >=1 and eval('loss_change_%d' % (i)) == 1:\n",
    "            #     globals()['X_%s' % i] = initialize_q_batch_nonneg(eval('Xraw_%d' % (i)), eval('Yraw_%d' % (i)), N)\n",
    "            #     counter += 1\n",
    "            # globals()['X_%s' % i] = initialize_q_batch_nonneg(eval('Xraw_%d' % (i)), eval('Yraw_%d' % (i)), N)\n",
    "            # eval('X_%d' % (i)).requires_grad_(True)  # require grad\n",
    "            # globals()['optimizer_%s' % i] = torch.optim.SGD([eval('X_%d' % (i))], lr=lr)  # optimizer, can use sgd or adam\n",
    "            globals()['X_traj_%s' % i] = []  # store sgd trajectory\n",
    "\n",
    "        index, element = max(enumerate(loss_vector), key=itemgetter(1))\n",
    "        loss_vector = []  # store losses (i.e., negative AF) from all devices\n",
    "        # -------------------\n",
    "        # running sgd\n",
    "        ## we assume all devices are connected and consensus step only happened once\n",
    "        ## we run multiple steps of local SGD\n",
    "        # -------------------\n",
    "\n",
    "        X_total = []\n",
    "        for i in range(0, num_dev):\n",
    "            X_temp = eval('X_%d' % (i)).detach().numpy()\n",
    "            X_total = np.append(X_total, X_temp)\n",
    "\n",
    "        if W_matrix_1[index, index] > ((num_dev-1)**2)/(n_comm*num_dev):\n",
    "            W_matrix_2 = W_matrix_1 - 1/(n_comm*num_dev)\n",
    "            W_matrix_2[:, index] = W_matrix_1[:, index] + (num_dev-1)/(n_comm*num_dev)\n",
    "            W_matrix_2[index, :] = W_matrix_1[index, :] + (num_dev - 1) / (n_comm * num_dev)\n",
    "            W_matrix_2[index, index] = W_matrix_1[index, index] - ((num_dev-1)**2)/(n_comm*num_dev)\n",
    "        else:\n",
    "            W_matrix_2 = np.zeros((num_dev,num_dev)) + (num_dev-2)/(num_dev-1)**2\n",
    "            W_matrix_2[:, index] = 1/(num_dev-1)\n",
    "            W_matrix_2[index, :] = 1/(num_dev-1)\n",
    "            W_matrix_2[index, index] = 0\n",
    "            print(\"change\")\n",
    "        W_matrix_2 = W_matrix_1 # delete \n",
    "        consensus = np.kron(W_matrix_2, I_matrix)  # W kron I\n",
    "        updated_X_total = np.matmul(consensus, X_total)  # consensus step\n",
    "        weight_increase = (num_dev-1)/(num_dev*n_comm)\n",
    "        W_matrix_increase = np.eye(num_dev)*weight_increase\n",
    "        weight_decrease = 1/(num_dev*n_comm)\n",
    "        W_matrix_decrease = np.ones((num_dev, num_dev)) * weight_decrease\n",
    "        np.fill_diagonal(W_matrix_decrease, 0)\n",
    "        W_matrix_1 = W_matrix_1 + W_matrix_increase - W_matrix_decrease\n",
    "        # consensus = np.kron(W_matrix_2, I_matrix)  # W kron I\n",
    "\n",
    "        for i in range(0, num_dev):\n",
    "\n",
    "            new_X = torch.from_numpy(updated_X_total[(i * input_dim): ((i + 1) * input_dim)])\n",
    "            new_X = new_X.reshape(1, input_dim)\n",
    "            temp_X = new_X\n",
    "            temp_X = temp_X + eval('c_%d' % (i))\n",
    "            new_Y = eval('a_%d' % (i))*Levy(dim=input_dim, negate=True)(temp_X).unsqueeze(-1) + eval('b_%d' % (i))\n",
    "\n",
    "            # globals()['train_X_%s' % i] = torch.cat((eval('temp_train_X_%d' % (i)), new_X), dim=0) ##\n",
    "            # globals()['train_Y_%s' % i] = torch.cat((eval('temp_train_Y_%d' % (i)), new_Y), dim=0) ##\n",
    "            # globals()['temp_train_X_%s' % i] = eval('train_X_%d' % (i))\n",
    "            # globals()['temp_train_Y_%s' % i] = eval('train_Y_%d' % (i))\n",
    "            # globals()['train_X_%s' % i] = normalize(eval('train_X_%d' % (i)), bounds=bounds)\n",
    "            globals()['train_X_%s' % i] = torch.cat((eval('train_X_%d' % (i)), new_X), dim=0)\n",
    "            globals()['train_Y_%s' % i] = torch.cat((eval('train_Y_%d' % (i)), new_Y), dim=0)\n",
    "\n",
    "            globals()['model_%s' % i] = SingleTaskGP(eval('train_X_%d' % (i)), eval('train_Y_%d' % (i)))\n",
    "            globals()['mll_%s' % i] = ExactMarginalLogLikelihood(eval('model_%d' % (i)).likelihood, eval('model_%d' % (i)))\n",
    "            fit_gpytorch_mll(eval('mll_%d' % (i)))\n",
    "            globals()['candidate_set_%s' % i] = bounds[0] + (bounds[1] - bounds[0]) * (\n",
    "                torch.rand(1000, bounds.size(1), device=bounds.device, dtype=bounds.dtype))\n",
    "            globals()['qMES_%s' % i] = ExpectedImprovement(eval('model_%d' % (i)), best_f= eval('train_Y_%d' % (i)).max()) # qMaxValueEntropy(eval('model_%d' % (i)), eval('candidate_set_%d' % (i)))\n",
    "\n",
    "            N = 1\n",
    "            q = 1\n",
    "            d = input_dim\n",
    "            globals()['Xraw_%s' % i] = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(100 * N, q, d)\n",
    "            globals()['Yraw_%s' % i] = eval('qMES_%d' % (i))(eval('Xraw_%d' % (i)))  # acquisition function\n",
    "            globals()['X_%s' % i], globals()['loss_%s' % i] = optimize_acqf(\n",
    "                eval('qMES_%d' % (i)), bounds=bounds, q=1, num_restarts=10, raw_samples=512,\n",
    "            )\n",
    "            loss_vector.append(-eval('loss_%d' % (i)).detach().numpy())\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "    average_gap = 0\n",
    "    for i in range(0, num_dev):\n",
    "        \n",
    "        globals()['end_opt_%s' % i] = eval('train_Y_%d' % (i)).max()\n",
    "        Gap = (eval('current_opt_%d' % (i))-eval('end_opt_%d' % (i)))/(eval('current_opt_%d' % (i))-eval('optimal_Y_%d' % (i)))\n",
    "        result.append(Gap)\n",
    "        average_gap += Gap\n",
    "        print(  Gap  )\n",
    "\n",
    "    # print(average_gap/num_dev)\n",
    "    # print(result)\n",
    "    # result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "consensus_BO_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
